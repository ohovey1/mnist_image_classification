{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Image Classification using CNN\n",
    "\n",
    "This project involves using deep neural networks to classify digits from the MNIST dataset. This task will be done by constructing a CNN using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the dataset, we'll define some transformations using torchvision.transforms.\n",
    "\n",
    "When the data is loaded in, it is normalized with a mean of 0.1307 and a standard deviation of 0.3081,which are specific to the MNIST dataset.\n",
    "\n",
    "Mean: 0.1307, represents the mean pixel intensity for the MNIST dataset\n",
    "\n",
    "Standard deviation: 0.3081, represents the standard deviation of the pixel intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert the image to a 3 dimensional tensor\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the image\n",
    "])\n",
    "\n",
    "# Load the training and test sets using transform\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define batch size and data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the classes\n",
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing some of the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Baseline Model\n",
    "\n",
    "Defining a fully connected network (MLP) to get a baseline performance. We'll then compare this to the CNN once it is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128) # First layer\n",
    "        self.fc2 = nn.Linear(128, 64) # Second layer\n",
    "        self.fc3 = nn.Linear(64, 10) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28) # Flattnes image to 1D vector\n",
    "        x = F.relu(self.fc1(x)) # ReLU activation function for first layer\n",
    "        x = F.relu(self.fc2(x)) # ReLU activation function for second layer\n",
    "        x = self.fc3(x) # Output layer, no activation\n",
    "        return x\n",
    "    \n",
    "# Instantiate model\n",
    "mlp = MLP().to(device)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Cross entropy for classification\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001) # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.2707\n",
      "Epoch [2/5], Loss: 0.1170\n",
      "Epoch [3/5], Loss: 0.0825\n",
      "Epoch [4/5], Loss: 0.0647\n",
      "Epoch [5/5], Loss: 0.0513\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predictions and loss\n",
    "        output = mlp(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass: compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving baseline model\n",
    "PATH = './mnist_mlp.pth'\n",
    "torch.save(mlp.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to test model\n",
    "def test_model(model, test_loader):\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Turn off gradients for validation\n",
    "        for data, target in test_loader:\n",
    "            # Move data to device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # Sum test loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # Get index of max logit\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0013, Test Accuracy: 97.42%\n"
     ]
    }
   ],
   "source": [
    "test_model(mlp, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Performance:** \n",
    "\n",
    "Test Loss: 0.0013\n",
    "\n",
    "Test Accuracy: 97.42%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=3136, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First conv layer (input: 1 channel, output: 32 channels, kernel size: 3x3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Max pooling layer (2x2 window), stride set at 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Second conv layer (input: 32 channels, output: 64 channels, kernel size: 3x3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Fully connected layer (64 channels * 7 * 7 image size)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "\n",
    "        # Second fully connected layer (128 input features, 64 output features)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # Output layer (64 input features, 10 output features)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # Conv layer 1, ReLU + pooling\n",
    "        x = self.pool(F.relu(self.conv2(x))) # Conv layer 2, ReLU + pooling\n",
    "        x = x.view(-1, 64 * 7 * 7) # Flatten image\n",
    "        x = F.relu(self.fc1(x)) # Fully connected layer 1, ReLU\n",
    "        x = F.relu(self.fc2(x)) # Fully connected layer 2, ReLU\n",
    "        x = self.fc3(x) # output layer\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "cnn = CNN().to(device)\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Cross entropy for classification\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001) # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.1596\n",
      "Epoch [2/5], Loss: 0.0489\n",
      "Epoch [3/5], Loss: 0.0344\n",
      "Epoch [4/5], Loss: 0.0247\n",
      "Epoch [5/5], Loss: 0.0198\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute predictions and loss\n",
    "        outputs = cnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute gradients and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "PATH = './mnist_cnn.pth'\n",
    "torch.save(cnn.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0006, Test Accuracy: 98.88%\n"
     ]
    }
   ],
   "source": [
    "# Test model, using the function defined earlier\n",
    "test_model(cnn, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
